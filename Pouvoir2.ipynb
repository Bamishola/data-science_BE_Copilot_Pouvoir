{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "170afab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes pr√©sentes dans test mais pas dans train : {\"tfidf_D√©s√©lection d'un √©l√©ment|||Ex√©cution d'un bouton\", \"tfidf_Affichage d'une dialogue|||Fermeture d'une dialogue\", \"tfidf_S√©lection d'un √©cran|||Dissimulation d'une arborescence\", \"tfidf_S√©lection d'un flag|||S√©lection d'un flag\", 'tfidf_Entr√©e en saisie dans un formulaire|||Retour sur un √©cran', \"tfidf_Retour sur un √©cran|||S√©lection d'un flag\", 'tfidf_Chainage|||Chainage', \"tfidf_D√©s√©lection d'un √©l√©ment|||Raccourci\", \"tfidf_Raccourci|||D√©s√©lection d'un √©l√©ment\", 'tfidf_Saisie dans un champ|||Saisie dans un champ', \"tfidf_D√©s√©lection d'un √©l√©ment|||Clic sur une grille d'historique de recherche\", \"tfidf_Lancement d'une stat|||Entr√©e en saisie dans un formulaire\", \"tfidf_Ex√©cution d'un bouton|||D√©s√©lection d'un √©l√©ment\", \"tfidf_S√©lection d'un √©cran|||Affichage d'une arborescence\", 'tfidf_Action de table|||Filtrage / Tri'}\n",
      "Colonnes pr√©sentes dans train mais pas dans test : {\"tfidf_Clic sur une checkbox|||S√©lection d'un √©cran\", \"tfidf_Affichage d'un toast|||Lancement d'une stat\", 'tfidf_Action de table|||Chainage', 'Y', \"tfidf_Affichage d'un toast|||Entr√©e en saisie dans un formulaire\", \"tfidf_Fermeture d'une dialogue|||Affichage d'une erreur\", 'tfidf_Saisie dans un champ|||Chainage', \"tfidf_Fermeture d'une dialogue|||Lancement d'une stat\", \"tfidf_Lancement d'une stat|||Lancement d'une stat\", \"tfidf_Fermeture d'un panel|||Ouverture d'un panel\", \"tfidf_Lancement d'une stat|||S√©lection d‚Äôun onglet\", \"tfidf_Lancement d'un tableau de bord|||Lancement d'un tableau de bord\", \"tfidf_Lancement d'une stat|||S√©lection d'un flag\", \"tfidf_Fermeture d'un panel|||Fermeture d'un panel\", 'tfidf_Action de table|||Raccourci', \"tfidf_Affichage d'un toast|||Clic sur une grille d'historique de recherche\"}\n",
      "Nombre de colonnes communes : 449\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charge les fichiers de features\n",
    "train_features = pd.read_csv(\"data/train_features_ready.csv\")\n",
    "test_features  = pd.read_csv(\"data/test_features_ready.csv\")\n",
    "\n",
    "# Colonnes\n",
    "train_cols = set(train_features.columns)\n",
    "test_cols  = set(test_features.columns)\n",
    "\n",
    "# Colonnes pr√©sentes dans test mais pas dans train\n",
    "extra_in_test = test_cols - train_cols\n",
    "print(\"Colonnes pr√©sentes dans test mais pas dans train :\", extra_in_test)\n",
    "\n",
    "# Colonnes pr√©sentes dans train mais pas dans test\n",
    "missing_in_test = train_cols - test_cols\n",
    "print(\"Colonnes pr√©sentes dans train mais pas dans test :\", missing_in_test)\n",
    "\n",
    "# Colonnes communes\n",
    "common_cols = train_cols & test_cols\n",
    "print(\"Nombre de colonnes communes :\", len(common_cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a90215",
   "metadata": {},
   "source": [
    "## permier code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baf06617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== BUILD_TRAIN_CLEAN ==========\n",
      "Input      : /home/bamishola/ECL/S9/UE - MOD/02 - MOD_07_2 - Introduction √† Data Science/BE 1 - 2 - 3/01-BE/data/train.csv\n",
      "Output     : train_clean.csv\n",
      "‚úÖ Lecture OK (sep=';')  lignes=3278  cols=1\n",
      "‚úÖ G√©n√©r√© : train_clean.csv  (3278 lignes, 3 colonnes)\n",
      "üßæ Colonnes finales : ['util', 'navigateur', 'trace_str']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BAD_TOKENS = {\"\", \"nan\", \"none\", \"null\"}\n",
    "\n",
    "# D√©finir les fichiers directement\n",
    "train = \"/home/bamishola/ECL/S9/UE - MOD/02 - MOD_07_2 - Introduction √† Data Science/BE 1 - 2 - 3/01-BE/data/train.csv\"\n",
    "input_file = Path(train)\n",
    "output_file = Path(\"train_clean.csv\")\n",
    "user_col_arg = None\n",
    "browser_col_arg = None\n",
    "\n",
    "# ... Toutes les fonctions du script (read_csv_any, normalize_headers, drop_useless_columns, etc.) ...\n",
    "\n",
    "# Adaptation de tableau_trois_colonne pour notebook\n",
    "print(\"========== BUILD_TRAIN_CLEAN ==========\")\n",
    "print(f\"Input      : {input_file}\")\n",
    "print(f\"Output     : {output_file}\")\n",
    "\n",
    "if not input_file.exists():\n",
    "    raise FileNotFoundError(f\"Fichier introuvable : {input_file}\")\n",
    "\n",
    "df, used_sep = read_csv_any(input_file)\n",
    "print(f\"‚úÖ Lecture OK (sep='{used_sep}')  lignes={len(df)}  cols={len(df.columns)}\")\n",
    "df = normalize_headers(df)\n",
    "\n",
    "# CAS 1 : une seule colonne\n",
    "if df.shape[1] == 1:\n",
    "    sole = df.columns[0]\n",
    "    out = parse_single_column_lines(df, sole)\n",
    "    out.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "    print(f\"‚úÖ G√©n√©r√© : {output_file}  ({len(out)} lignes, {len(out.columns)} colonnes)\")\n",
    "    print(\"üßæ Colonnes finales :\", list(out.columns))\n",
    "else:\n",
    "    df = drop_useless_columns(df)\n",
    "    cols = df.columns.tolist()\n",
    "    print(\"üß≠ Colonnes (extrait) :\", cols[:20], \"...\" if len(cols) > 20 else \"\")\n",
    "\n",
    "    user_col = user_col_arg or find_col_exact(USER_COL_CANDIDATES, cols) or find_col_fuzzy(cols, [\"util\", \"user\", \"ident\", \"id\"])\n",
    "    nav_col = browser_col_arg or find_col_exact(BROWSER_COL_CANDIDATES, cols) or find_col_fuzzy(cols, [\"navig\", \"browser\", \"nav\", \"agent\"])\n",
    "\n",
    "    print(f\"üë§ Colonne utilisateur : {user_col if user_col else '(absente)'}\")\n",
    "    print(f\"üåê Colonne navigateur : {nav_col if nav_col else '(introuvable ‚Üí sera \\\"unknown\\\")'}\")\n",
    "\n",
    "    exclude = set(filter(None, [user_col, nav_col]))\n",
    "    action_cols = [c for c in cols if c not in exclude]\n",
    "    if not action_cols:\n",
    "        raise RuntimeError(\"Aucune colonne d'actions d√©tect√©e (tout est user/nav ?)\")\n",
    "\n",
    "    def row_to_trace(row):\n",
    "        toks = []\n",
    "        for c in action_cols:\n",
    "            v = row.get(c, None)\n",
    "            if isinstance(v, str):\n",
    "                vv = v.strip()\n",
    "                if vv and vv.lower() not in BAD_TOKENS:\n",
    "                    toks.append(vv)\n",
    "        return \", \".join(toks)\n",
    "\n",
    "    trace_str = df[action_cols].apply(row_to_trace, axis=1)\n",
    "    out = pd.DataFrame({\"trace_str\": trace_str})\n",
    "    out.insert(0, \"navigateur\", df[nav_col].astype(str).fillna(\"unknown\") if nav_col else \"unknown\")\n",
    "    if user_col:\n",
    "        out.insert(0, \"util\", df[user_col].astype(str))\n",
    "\n",
    "    out.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "    print(f\"‚úÖ G√©n√©r√© : {output_file}  ({len(out)} lignes, {len(out.columns)} colonnes)\")\n",
    "    print(\"üßæ Colonnes finales :\", list(out.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ec73e0",
   "metadata": {},
   "source": [
    "## 2ieme code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aab5f787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ D√©marrage g√©n√©ration de features (mode VS Code)\n",
      "üìä Dimensions avant transformation : 3278 lignes √ó 3 colonnes\n",
      "üìå Fit artefacts‚Ä¶\n",
      "üîÑ Rechargement artefacts‚Ä¶\n",
      "üõ† Transformation‚Ä¶\n",
      "‚úÖ Dimensions apr√®s transformation : 3278 lignes √ó 465 colonnes\n",
      "üíæ Fichier de sortie enregistr√© : train_features_ready.csv\n",
      "üìÅ Artefacts : ./artifacts_vscode\n"
     ]
    }
   ],
   "source": [
    "# features_extraction.py\n",
    "# ------------------------------------------------------------\n",
    "# Extraction des features √† partir de train_clean.csv :\n",
    "# - TF-IDF sur LISTES d'actions (unigram + bigram) via analyzer callable (niveau module, picklable)\n",
    "# - Topics SVD(20) sur TF-IDF\n",
    "# - Features s√©quentielles / temporelles / ratios s√©mantiques\n",
    "# - Divisions prot√©g√©es (pas de RuntimeWarning)\n",
    "# - Artefacts s√©rialis√©s (TF-IDF, SVD, vocabs...)\n",
    "# Sortie : train_features_ready.csv\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass, asdict\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# ===================== PARAMS =====================\n",
    "TRANSFORM_ONLY = False\n",
    "INPUT_FILE = \"train_clean.csv\"\n",
    "OUTPUT_FILE = \"train_features_ready.csv\"\n",
    "ARTIFACTS_DIR = \"./artifacts_vscode\"\n",
    "\n",
    "SECONDS_PER_TICK = 5\n",
    "TOP_K_ACTIONS = 36\n",
    "TFIDF_MAX_FEATURES = 400\n",
    "TFIDF_NGRAM_MAX = 2\n",
    "MIN_NAV_CAT_FREQ_RATIO = 0.01\n",
    "ADD_NAV_INTERACTIONS = True\n",
    "TOP_K_ACTION_BIGRAMS = 20\n",
    "SVD_N_COMPONENTS = 20\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "logger = logging.getLogger(\"features\")\n",
    "\n",
    "# ----------------------- Helpers -----------------------\n",
    "def nettoie_action(token: str) -> str:\n",
    "    if not isinstance(token, str) or token == \"\":\n",
    "        return \"\"\n",
    "    for sep in [\"(\", \"<\", \"$\", \"1\"]:\n",
    "        if sep in token:\n",
    "            i = token.index(sep)\n",
    "            if i > 0:\n",
    "                token = token[:i]\n",
    "    token = re.sub(r\"\\s+\", \" \", token).strip()\n",
    "    return token\n",
    "\n",
    "def decoupe_trace(trace: str) -> List[str]:\n",
    "    if not isinstance(trace, str) or trace == \"\":\n",
    "        return []\n",
    "    return [t.strip() for t in trace.split(\",\") if t.strip()]\n",
    "\n",
    "def actions_nettoyees_sans_t(trace_str: str) -> List[str]:\n",
    "    acts = []\n",
    "    for tok in decoupe_trace(trace_str):\n",
    "        if tok.startswith(\"t\"):\n",
    "            continue\n",
    "        a = nettoie_action(tok)\n",
    "        if a:\n",
    "            acts.append(a)\n",
    "    return acts\n",
    "\n",
    "def entropy_from_counts(counts: Counter) -> float:\n",
    "    total = sum(counts.values())\n",
    "    if total <= 0:\n",
    "        return 0.0\n",
    "    ent = 0.0\n",
    "    for c in counts.values():\n",
    "        p = c / total\n",
    "        if p > 0:\n",
    "            ent -= p * math.log(p)\n",
    "    return float(ent)\n",
    "\n",
    "def top2_ratio_from_counts(cnt: Counter, n_total: int) -> float:\n",
    "    if n_total <= 0 or not cnt:\n",
    "        return 0.0\n",
    "    top2 = sorted(cnt.values(), reverse=True)[:2]\n",
    "    return float(sum(top2) / n_total)\n",
    "\n",
    "def gini_from_counts(cnt: Counter) -> float:\n",
    "    total = sum(cnt.values())\n",
    "    if total <= 0:\n",
    "        return 0.0\n",
    "    p2 = sum((c / total) ** 2 for c in cnt.values())\n",
    "    return float(1.0 - p2)\n",
    "\n",
    "def simpson_from_counts(cnt: Counter) -> float:\n",
    "    total = sum(cnt.values())\n",
    "    if total <= 0:\n",
    "        return 0.0\n",
    "    return float(sum((c / total) ** 2 for c in cnt.values()))\n",
    "\n",
    "def plus_frequent(pattern: re.Pattern, texte: str) -> Optional[str]:\n",
    "    if not isinstance(texte, str) or not texte:\n",
    "        return None\n",
    "    matches = pattern.findall(texte)\n",
    "    return Counter(matches).most_common(1)[0][0] if matches else None\n",
    "\n",
    "def count_repeats(actions: List[str]) -> int:\n",
    "    \"\"\"Nombre de r√©p√©titions cons√©cutives (runs-1 cumul√©s).\"\"\"\n",
    "    if not actions:\n",
    "        return 0\n",
    "    repeats = 0\n",
    "    prev = actions[0]\n",
    "    run = 1\n",
    "    for a in actions[1:]:\n",
    "        if a == prev:\n",
    "            run += 1\n",
    "        else:\n",
    "            if run > 1:\n",
    "                repeats += (run - 1)\n",
    "            prev = a\n",
    "            run = 1\n",
    "    if run > 1:\n",
    "        repeats += (run - 1)\n",
    "    return repeats\n",
    "\n",
    "def actions_per_tick(trace_str: str) -> List[int]:\n",
    "    \"\"\"Compte nb d'actions (hors t*) entre chaque tN.\"\"\"\n",
    "    toks = decoupe_trace(trace_str)\n",
    "    counts = []\n",
    "    c = 0\n",
    "    saw_any_t = False\n",
    "    for tok in toks:\n",
    "        if tok.startswith(\"t\"):\n",
    "            saw_any_t = True\n",
    "            counts.append(c)\n",
    "            c = 0\n",
    "        else:\n",
    "            if nettoie_action(tok):\n",
    "                c += 1\n",
    "    if not saw_any_t:\n",
    "        return [c] if (c > 0 or len(toks) > 0) else []\n",
    "    counts.append(c)\n",
    "    return counts\n",
    "\n",
    "def bigrams_from_actions(actions: List[str]) -> List[str]:\n",
    "    if len(actions) < 2:\n",
    "        return []\n",
    "    return [f\"{actions[i]}|||{actions[i+1]}\" for i in range(len(actions) - 1)]\n",
    "\n",
    "# === Analyzer callable picklable (NIVEAU MODULE) ===\n",
    "def ngram_actions(tokens: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Re√ßoit une LISTE d'actions ; renvoie unigrams + bigrams (s√©parateur '|||').\n",
    "    D√©finie au niveau module => picklable par joblib.\n",
    "    \"\"\"\n",
    "    if not isinstance(tokens, list):\n",
    "        return []\n",
    "    if len(tokens) < 2:\n",
    "        return tokens[:]\n",
    "    bigrams = [f\"{tokens[i]}|||{tokens[i+1]}\" for i in range(len(tokens) - 1)]\n",
    "    return tokens + bigrams\n",
    "\n",
    "# ----------------------- Config & Artefacts -----------------------\n",
    "@dataclass\n",
    "class FeatureConfig:\n",
    "    seconds_per_tick: int = SECONDS_PER_TICK\n",
    "    top_k_actions: int = TOP_K_ACTIONS\n",
    "    tfidf_max_features: int = TFIDF_MAX_FEATURES\n",
    "    tfidf_ngram_min: int = 1\n",
    "    tfidf_ngram_max: int = TFIDF_NGRAM_MAX\n",
    "    min_cat_freq_ratio: float = MIN_NAV_CAT_FREQ_RATIO\n",
    "    add_nav_interactions: bool = ADD_NAV_INTERACTIONS\n",
    "    svd_n_components: int = SVD_N_COMPONENTS\n",
    "\n",
    "@dataclass\n",
    "class ArtifactsSummary:\n",
    "    version: str\n",
    "    config: dict\n",
    "    nav_vocab: List[str]\n",
    "    cat_maps: Dict[str, Dict[str, int]]\n",
    "    label_map_present: bool\n",
    "    tfidf_vectorizer_path: str\n",
    "    svd_path: str\n",
    "    top_actions: List[str]\n",
    "    outlier_nb_actions_threshold: int\n",
    "\n",
    "# ----------------------- Assembleur -----------------------\n",
    "class FeaturesAssembler:\n",
    "    def __init__(self, config: FeatureConfig):\n",
    "        self.cfg = config\n",
    "        self._pat_ecran = re.compile(r\"\\((.*?)\\)\")\n",
    "        self._pat_conf = re.compile(r\"<(.*?)>\")\n",
    "        self._pat_chaine = re.compile(r\"\\$(.*?)\\$\")\n",
    "        self._tfidf: Optional[TfidfVectorizer] = None\n",
    "        self._svd: Optional[TruncatedSVD] = None\n",
    "        self._nav_vocab: List[str] = []\n",
    "        self._cat_maps: Dict[str, Dict[str, int]] = {}\n",
    "        self._label_map: Optional[Dict[str, int]] = None\n",
    "        self._top_actions: List[str] = []\n",
    "        self._outlier_thr: int = 0\n",
    "        self._first_freq_map: Dict[str, float] = {}\n",
    "        self._second_freq_map: Dict[str, float] = {}\n",
    "        self._bigram_id_map: Dict[str, int] = {}\n",
    "\n",
    "    # -------- FIT ----------\n",
    "    def fit(self, df: pd.DataFrame, artifacts_dir: str):\n",
    "        os.makedirs(artifacts_dir, exist_ok=True)\n",
    "\n",
    "        # Label map\n",
    "        if \"util\" in df.columns:\n",
    "            cat = pd.Categorical(df[\"util\"].fillna(\"\").astype(str))\n",
    "            self._label_map = {cat.categories[i]: int(i) for i in range(len(cat.categories))}\n",
    "            pd.DataFrame({\"util\": cat.categories, \"Y\": range(len(cat.categories))}).to_csv(\n",
    "                os.path.join(artifacts_dir, \"label_mapping.csv\"), index=False, encoding=\"utf-8\"\n",
    "            )\n",
    "        else:\n",
    "            self._label_map = None\n",
    "\n",
    "        # Vocab navigateur\n",
    "        if \"navigateur\" in df.columns:\n",
    "            counts = df[\"navigateur\"].fillna(\"\").astype(str).value_counts(dropna=False)\n",
    "            thr = max(1, int(self.cfg.min_cat_freq_ratio * len(df)))\n",
    "            keep = counts[counts >= thr].index.tolist()\n",
    "            if len(keep) < len(counts):\n",
    "                keep.append(\"other\")\n",
    "            self._nav_vocab = keep\n",
    "            with open(os.path.join(artifacts_dir, \"nav_vocab.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(self._nav_vocab, f, ensure_ascii=False)\n",
    "        else:\n",
    "            self._nav_vocab = []\n",
    "\n",
    "        # Cat maps (entit√©s extraites)\n",
    "        ecran_raw  = df[\"trace_str\"].apply(lambda x: plus_frequent(self._pat_ecran, x))\n",
    "        conf_raw   = df[\"trace_str\"].apply(lambda x: plus_frequent(self._pat_conf, x))\n",
    "        chaine_raw = df[\"trace_str\"].apply(lambda x: plus_frequent(self._pat_chaine, x))\n",
    "        def learn_cmap(series: pd.Series) -> Dict[str, int]:\n",
    "            categories = pd.Categorical(series.fillna(\"\").astype(str)).categories.tolist()\n",
    "            return {c: i for i, c in enumerate(categories)}\n",
    "        self._cat_maps = {\n",
    "            \"ecran\":  learn_cmap(ecran_raw),\n",
    "            \"conf\":   learn_cmap(conf_raw),\n",
    "            \"chaine\": learn_cmap(chaine_raw),\n",
    "        }\n",
    "        with open(os.path.join(artifacts_dir, \"cat_maps.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._cat_maps, f, ensure_ascii=False)\n",
    "\n",
    "        # Top-K actions globales + documents (listes d'actions)\n",
    "        compteur_global = Counter()\n",
    "        action_docs = []\n",
    "        for trace in df[\"trace_str\"].astype(str):\n",
    "            acts = actions_nettoyees_sans_t(trace)\n",
    "            action_docs.append(acts)\n",
    "            for a in acts:\n",
    "                compteur_global[a] += 1\n",
    "        self._top_actions = [a for a, _ in Counter(compteur_global).most_common(self.cfg.top_k_actions)]\n",
    "        with open(os.path.join(artifacts_dir, \"top_actions.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._top_actions, f, ensure_ascii=False)\n",
    "\n",
    "        # ===== TF-IDF (analyzer callable niveau module) =====\n",
    "        self._tfidf = TfidfVectorizer(\n",
    "            analyzer=ngram_actions,   # <‚Äî picklable\n",
    "            preprocessor=None,\n",
    "            tokenizer=None,\n",
    "            lowercase=False,\n",
    "            min_df=3,\n",
    "            max_df=0.90,\n",
    "            max_features=self.cfg.tfidf_max_features,\n",
    "            sublinear_tf=True\n",
    "        )\n",
    "        self._tfidf.fit(action_docs)\n",
    "        tfidf_path = os.path.join(artifacts_dir, \"tfidf_vectorizer.joblib\")\n",
    "        joblib.dump(self._tfidf, tfidf_path)\n",
    "\n",
    "        # ===== SVD topics =====\n",
    "        tfidf_fit = self._tfidf.transform(action_docs)\n",
    "        self._svd = TruncatedSVD(n_components=self.cfg.svd_n_components, random_state=42)\n",
    "        self._svd.fit(tfidf_fit)\n",
    "        svd_path = os.path.join(artifacts_dir, f\"svd_{self.cfg.svd_n_components}.joblib\")\n",
    "        joblib.dump(self._svd, svd_path)\n",
    "\n",
    "        # Fr√©quences 1√®re/2√®me action\n",
    "        first_actions  = [acts[0] for acts in action_docs if len(acts) > 0]\n",
    "        second_actions = [acts[1] for acts in action_docs if len(acts) > 1]\n",
    "        def freq_map(lst: List[str]) -> Dict[str, float]:\n",
    "            cnt = Counter(lst); tot = sum(cnt.values())\n",
    "            return {k: v / tot for k, v in cnt.items()} if tot > 0 else {}\n",
    "        self._first_freq_map  = freq_map(first_actions)\n",
    "        self._second_freq_map = freq_map(second_actions)\n",
    "        with open(os.path.join(artifacts_dir, \"first_action_freq.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._first_freq_map, f, ensure_ascii=False)\n",
    "        with open(os.path.join(artifacts_dir, \"second_action_freq.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._second_freq_map, f, ensure_ascii=False)\n",
    "\n",
    "        # Bigrammes d'actions compress√©s (ID)\n",
    "        bigram_cnt = Counter()\n",
    "        for acts in action_docs:\n",
    "            bigram_cnt.update(bigrams_from_actions(acts))\n",
    "        top_bigrams = [bg for bg, _ in bigram_cnt.most_common(TOP_K_ACTION_BIGRAMS)]\n",
    "        self._bigram_id_map = {bg: i + 1 for i, bg in enumerate(top_bigrams)}  # 0 = autre/absent\n",
    "        with open(os.path.join(artifacts_dir, \"top_action_bigrams.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._bigram_id_map, f, ensure_ascii=False)\n",
    "\n",
    "        # Seuil outlier\n",
    "        nb_actions_arr = np.where(df[\"trace_str\"].astype(str) == \"\", 0, df[\"trace_str\"].astype(str).str.count(\",\") + 1)\n",
    "        self._outlier_thr = int(pd.Series(nb_actions_arr).quantile(0.99)) if len(df) > 0 else 0\n",
    "        with open(os.path.join(artifacts_dir, \"outlier_thr.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"nb_actions_q99\": self._outlier_thr}, f, ensure_ascii=False)\n",
    "\n",
    "        # R√©sum√© artefacts\n",
    "        summary = ArtifactsSummary(\n",
    "            version=\"1.0.0\",\n",
    "            config=asdict(self.cfg),\n",
    "            nav_vocab=self._nav_vocab,\n",
    "            cat_maps=self._cat_maps,\n",
    "            label_map_present=self._label_map is not None,\n",
    "            tfidf_vectorizer_path=tfidf_path,\n",
    "            svd_path=svd_path,\n",
    "            top_actions=self._top_actions,\n",
    "            outlier_nb_actions_threshold=self._outlier_thr\n",
    "        )\n",
    "        with open(os.path.join(artifacts_dir, \"artifacts_summary.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(asdict(summary), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # --- LOAD ---\n",
    "    def load(self, artifacts_dir: str):\n",
    "        self._tfidf = joblib.load(os.path.join(artifacts_dir, \"tfidf_vectorizer.joblib\"))\n",
    "        svd_path = os.path.join(artifacts_dir, f\"svd_{self.cfg.svd_n_components}.joblib\")\n",
    "        self._svd = joblib.load(svd_path) if os.path.exists(svd_path) else None\n",
    "\n",
    "        with open(os.path.join(artifacts_dir, \"nav_vocab.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            self._nav_vocab = json.load(f)\n",
    "        with open(os.path.join(artifacts_dir, \"cat_maps.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            self._cat_maps = json.load(f)\n",
    "        with open(os.path.join(artifacts_dir, \"top_actions.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "            self._top_actions = json.load(f)\n",
    "\n",
    "        first_path  = os.path.join(artifacts_dir, \"first_action_freq.json\")\n",
    "        second_path = os.path.join(artifacts_dir, \"second_action_freq.json\")\n",
    "        bigram_path = os.path.join(artifacts_dir, \"top_action_bigrams.json\")\n",
    "        if os.path.exists(first_path):\n",
    "            with open(first_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                self._first_freq_map = json.load(f)\n",
    "        if os.path.exists(second_path):\n",
    "            with open(second_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                self._second_freq_map = json.load(f)\n",
    "        if os.path.exists(bigram_path):\n",
    "            with open(bigram_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                self._bigram_id_map = json.load(f)\n",
    "\n",
    "        thr_path = os.path.join(artifacts_dir, \"outlier_thr.json\")\n",
    "        if os.path.exists(thr_path):\n",
    "            with open(thr_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                self._outlier_thr = json.load(f).get(\"nb_actions_q99\", 0)\n",
    "\n",
    "        label_map_path = os.path.join(artifacts_dir, \"label_mapping.csv\")\n",
    "        if os.path.exists(label_map_path):\n",
    "            df_map = pd.read_csv(label_map_path, dtype=str)\n",
    "            self._label_map = {row[\"util\"]: int(row[\"Y\"]) for _, row in df_map.iterrows()}\n",
    "        else:\n",
    "            self._label_map = None\n",
    "\n",
    "    # -------- TRANSFORM ----------\n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        trace = df[\"trace_str\"].astype(str).fillna(\"\")\n",
    "        nb_actions = np.where(trace == \"\", 0, trace.str.count(\",\") + 1).astype(int)\n",
    "        nb_time_tokens = trace.str.count(r\"\\bt\\d+\\b\").astype(int)\n",
    "\n",
    "        # t_max / dur√©e\n",
    "        extra_t = trace.str.extractall(r\"\\bt(\\d+)\\b\")\n",
    "        if not extra_t.empty:\n",
    "            tmax_par_ligne = extra_t[0].astype(int).groupby(level=0).max()\n",
    "            t_max = df.index.to_series().map(tmax_par_ligne).fillna(0).astype(int).values\n",
    "        else:\n",
    "            t_max = np.zeros(len(df), dtype=int)\n",
    "        session_duration_sec = (self.cfg.seconds_per_tick * t_max).astype(int)\n",
    "\n",
    "        # Actions & counts\n",
    "        actions_lists = trace.apply(actions_nettoyees_sans_t)\n",
    "        counts_lists = actions_lists.apply(Counter)\n",
    "        n_act_clean = actions_lists.apply(len).astype(int)\n",
    "\n",
    "        # Rythme\n",
    "        dur = session_duration_sec.astype(float)\n",
    "        rate = np.where(dur <= 0, 0.0, 60.0 * n_act_clean.values / np.clip(dur, 1e-6, None))\n",
    "\n",
    "        # Entropie actions\n",
    "        action_entropy = counts_lists.apply(entropy_from_counts).astype(float).values\n",
    "        denom = np.log(np.clip(n_act_clean.values.astype(float), 1.0, None) + 1e-6)\n",
    "        action_entropy_norm = np.where(denom > 0, action_entropy / denom, 0.0)\n",
    "\n",
    "        # Diversit√© & concentration\n",
    "        def _top_ratio(cnt: Counter, n_total: int):\n",
    "            return float(max(cnt.values()) / n_total) if n_total > 0 and cnt else 0.0\n",
    "        top_action_ratio = np.array([_top_ratio(c, n) for c, n in zip(counts_lists, n_act_clean)], dtype=float)\n",
    "        top2_ratio = np.array([top2_ratio_from_counts(c, n) for c, n in zip(counts_lists, n_act_clean)], dtype=float)\n",
    "        n_actions_uniques = counts_lists.apply(len).astype(int).values\n",
    "        gini = np.array([gini_from_counts(c) for c in counts_lists], dtype=float)\n",
    "        simpson = np.array([simpson_from_counts(c) for c in counts_lists], dtype=float)\n",
    "\n",
    "        # Coverage TF (masqu√©)\n",
    "        top_set = set(self._top_actions or [])\n",
    "        vocab_covered = actions_lists.apply(lambda xs: sum(1 for x in xs if x in top_set)).astype(float).values\n",
    "        n_total = n_act_clean.values.astype(float)\n",
    "        coverage_rate = np.zeros_like(n_total, dtype=float)\n",
    "        mask_cov = n_total > 0\n",
    "        coverage_rate[mask_cov] = vocab_covered[mask_cov] / n_total[mask_cov]\n",
    "        unknown_rate = 1.0 - coverage_rate\n",
    "\n",
    "        # Ratios s√©mantiques\n",
    "        texts_joined = actions_lists.apply(lambda toks: \" \".join(toks)).astype(str)\n",
    "        nb_ecrans = texts_joined.str.count(\"ecran\").values\n",
    "        nb_boutons = texts_joined.str.count(\"bouton\").values\n",
    "        nb_dialogues = texts_joined.str.count(\"dialogue\").values\n",
    "        nb_saisie = texts_joined.str.count(\"saisie\").values\n",
    "        nb_navigation = (\n",
    "            texts_joined.str.count(\"chainage\") +\n",
    "            texts_joined.str.count(\"onglet\") +\n",
    "            texts_joined.str.count(\"retour sur un ecran\") +\n",
    "            texts_joined.str.count(\"navigation\")\n",
    "        ).values\n",
    "        nb_raccourcis = texts_joined.str.count(\"raccourci\").values\n",
    "\n",
    "        def safe_ratio(x, total):\n",
    "            total = np.asarray(total, dtype=float)\n",
    "            out = np.zeros_like(total, dtype=float)\n",
    "            mask = total > 0\n",
    "            out[mask] = np.asarray(x, dtype=float)[mask] / total[mask]\n",
    "            return out\n",
    "\n",
    "        ratio_ecrans = safe_ratio(nb_ecrans, n_act_clean.values)\n",
    "        ratio_boutons = safe_ratio(nb_boutons, n_act_clean.values)\n",
    "        ratio_dialogues = safe_ratio(nb_dialogues, n_act_clean.values)\n",
    "        ratio_saisie = safe_ratio(nb_saisie, n_act_clean.values)\n",
    "        ratio_navigation = safe_ratio(nb_navigation, n_act_clean.values)\n",
    "        ratio_raccourcis = safe_ratio(nb_raccourcis, n_act_clean.values)\n",
    "\n",
    "        # Bins de dur√©e\n",
    "        session_length_bin = np.where(t_max <= 6, 0,\n",
    "                                np.where(t_max <= 24, 1,\n",
    "                                np.where(t_max <= 60, 2, 3))).astype(int)\n",
    "\n",
    "        # R√©p√©titions cons√©cutives\n",
    "        nb_repeat_sequences = actions_lists.apply(count_repeats).astype(int).values\n",
    "        repeat_ratio = safe_ratio(nb_repeat_sequences, n_act_clean.values)\n",
    "\n",
    "        # Retour arri√®re\n",
    "        def back_count(xs: List[str]) -> int:\n",
    "            if not xs: return 0\n",
    "            c = 0\n",
    "            for a in xs:\n",
    "                a_low = a.lower()\n",
    "                if (\"retour\" in a_low) or (\"back\" in a_low) or (\"arriere\" in a_low) or (\"pr√©c√©dent\" in a_low) or (\"precedent\" in a_low):\n",
    "                    c += 1\n",
    "            return c\n",
    "        nb_back_actions = actions_lists.apply(back_count).astype(int).values\n",
    "        back_action_ratio = safe_ratio(nb_back_actions, n_act_clean.values)\n",
    "\n",
    "        # Position 1√®re/2√®me action\n",
    "        first_actions = actions_lists.apply(lambda xs: xs[0] if len(xs) > 0 else \"\").values\n",
    "        second_actions = actions_lists.apply(lambda xs: xs[1] if len(xs) > 1 else \"\").values\n",
    "        first_action_freq = np.array([self._first_freq_map.get(str(a), 0.0) for a in first_actions], dtype=float)\n",
    "        second_action_freq = np.array([self._second_freq_map.get(str(a), 0.0) for a in second_actions], dtype=float)\n",
    "\n",
    "        # Bigramme compress√© (ID top)\n",
    "        def top_bigram_id(xs: List[str]) -> int:\n",
    "            if len(xs) < 2 or not self._bigram_id_map:\n",
    "                return 0\n",
    "            bgs = bigrams_from_actions(xs)\n",
    "            cnt = Counter(bgs)\n",
    "            for bg, _ in cnt.most_common():\n",
    "                if bg in self._bigram_id_map:\n",
    "                    return int(self._bigram_id_map[bg])\n",
    "            return 0\n",
    "        top_bigram_ids = actions_lists.apply(top_bigram_id).astype(int).values\n",
    "\n",
    "        # Fluidit√© temporelle\n",
    "        def mean_var_actions_per_tick(ts: str) -> Tuple[float, float]:\n",
    "            cps = actions_per_tick(ts)\n",
    "            if len(cps) == 0:\n",
    "                return 0.0, 0.0\n",
    "            arr = np.array(cps, dtype=float)\n",
    "            return float(arr.mean()), float(arr.var())\n",
    "        mv = trace.apply(mean_var_actions_per_tick)\n",
    "        avg_actions_per_tick = np.array([m for (m, v) in mv], dtype=float)\n",
    "        var_actions_per_tick = np.array([v for (m, v) in mv], dtype=float)\n",
    "\n",
    "        # std & burstiness\n",
    "        def mean_std_actions_per_tick(ts: str) -> Tuple[float, float]:\n",
    "            cps = actions_per_tick(ts)\n",
    "            if len(cps) == 0:\n",
    "                return 0.0, 0.0\n",
    "            arr = np.array(cps, dtype=float)\n",
    "            return float(arr.mean()), float(arr.std(ddof=0))\n",
    "        ms = trace.apply(mean_std_actions_per_tick)\n",
    "        mean_actions_per_tick = np.array([m for (m, s) in ms], dtype=float)\n",
    "        std_actions_per_tick  = np.array([s for (m, s) in ms], dtype=float)\n",
    "        burstiness_actions = np.where(\n",
    "            (mean_actions_per_tick + std_actions_per_tick) > 0,\n",
    "            (std_actions_per_tick - mean_actions_per_tick) / (std_actions_per_tick + mean_actions_per_tick + 1e-6),\n",
    "            0.0\n",
    "        )\n",
    "\n",
    "        # Entit√©s fr√©quentes (plus fr√©quent)\n",
    "        ecran_raw  = trace.apply(lambda x: plus_frequent(self._pat_ecran, x)).astype(object)\n",
    "        conf_raw   = trace.apply(lambda x: plus_frequent(self._pat_conf, x)).astype(object)\n",
    "        chaine_raw = trace.apply(lambda x: plus_frequent(self._pat_chaine, x)).astype(object)\n",
    "        def map_cat(raw: pd.Series, cmap: Dict[str, int]):\n",
    "            return raw.apply(lambda x: cmap.get(\"\" if x is None else str(x), -1)).astype(int).values\n",
    "        ecran_plus_frequent  = map_cat(ecran_raw,  self._cat_maps[\"ecran\"])\n",
    "        conf_plus_frequent   = map_cat(conf_raw,   self._cat_maps[\"conf\"])\n",
    "        chaine_plus_frequent = map_cat(chaine_raw, self._cat_maps[\"chaine\"])\n",
    "\n",
    "        # Nav\n",
    "        if \"navigateur\" in df.columns and self._nav_vocab:\n",
    "            nav_raw = df[\"navigateur\"].fillna(\"\").astype(str).values\n",
    "            known = set(self._nav_vocab)\n",
    "            has_other = \"other\" in known\n",
    "            nav_mapped = np.array([v if v in known else (\"other\" if has_other else v) for v in nav_raw], dtype=object)\n",
    "            nav_cols = [f\"nav_{c}\" for c in self._nav_vocab]\n",
    "            nav_matrix = np.zeros((len(df), len(self._nav_vocab)), dtype=int)\n",
    "            col_idx = {c: i for i, c in enumerate(self._nav_vocab)}\n",
    "            for i, v in enumerate(nav_mapped):\n",
    "                j = col_idx.get(v, None)\n",
    "                if j is not None:\n",
    "                    nav_matrix[i, j] = 1\n",
    "            nav_df = pd.DataFrame(nav_matrix, columns=nav_cols, index=df.index)\n",
    "        else:\n",
    "            nav_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "        if self.cfg.add_nav_interactions and not nav_df.empty:\n",
    "            for c in list(nav_df.columns):\n",
    "                nav_df[f\"{c}__x_rate\"] = nav_df[c].values * rate\n",
    "\n",
    "        # ===== TF-IDF + Topics =====\n",
    "        action_docs = actions_lists.tolist()         # liste de LISTES\n",
    "        tfidf_mat = self._tfidf.transform(action_docs)\n",
    "        tfidf_cols = [f\"tfidf_{t}\" for t in self._tfidf.get_feature_names_out()]\n",
    "        tfidf_df = pd.DataFrame.sparse.from_spmatrix(tfidf_mat, index=df.index, columns=tfidf_cols)\n",
    "\n",
    "        if self._svd is not None:\n",
    "            tfidf_topics = self._svd.transform(tfidf_mat)\n",
    "            topic_df = pd.DataFrame(\n",
    "                {f\"tfidf_topic_{i+1}\": tfidf_topics[:, i] for i in range(tfidf_topics.shape[1])},\n",
    "                index=df.index\n",
    "            )\n",
    "        else:\n",
    "            topic_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "        is_outlier = (nb_actions > (self._outlier_thr or 0)).astype(int)\n",
    "\n",
    "        base_df = pd.DataFrame({\n",
    "            \"nb_actions\": nb_actions,\n",
    "            \"nb_time_tokens\": nb_time_tokens,\n",
    "            \"t_max\": t_max,\n",
    "            \"action_rate_per_min\": rate,\n",
    "            \"action_entropy\": action_entropy,\n",
    "            \"action_entropy_norm\": action_entropy_norm,\n",
    "            \"top_action_ratio\": top_action_ratio,\n",
    "            \"top2_ratio\": top2_ratio,\n",
    "            \"ratio_ecrans\": ratio_ecrans,\n",
    "            \"ratio_boutons\": ratio_boutons,\n",
    "            \"ratio_dialogues\": ratio_dialogues,\n",
    "            \"ratio_saisie\": ratio_saisie,\n",
    "            \"ratio_navigation\": ratio_navigation,\n",
    "            \"ratio_raccourcis\": ratio_raccourcis,\n",
    "            \"n_actions_uniques\": n_actions_uniques,\n",
    "            \"gini_diversity\": gini,\n",
    "            \"simpson_concentration\": simpson,\n",
    "            \"coverage_rate\": coverage_rate,\n",
    "            \"unknown_rate\": unknown_rate,\n",
    "            \"ecran_plus_frequent\": ecran_plus_frequent,\n",
    "            \"conf_plus_frequent\": conf_plus_frequent,\n",
    "            \"chaine_plus_frequent\": chaine_plus_frequent,\n",
    "            \"is_outlier_nb_actions_q99\": is_outlier,\n",
    "            \"session_length_bin\": session_length_bin,\n",
    "            \"nb_repeat_sequences\": nb_repeat_sequences,\n",
    "            \"repeat_ratio\": repeat_ratio,\n",
    "            \"nb_back_actions\": nb_back_actions,\n",
    "            \"back_action_ratio\": back_action_ratio,\n",
    "            \"first_action_freq\": first_action_freq,\n",
    "            \"second_action_freq\": second_action_freq,\n",
    "            \"top_bigram_id\": top_bigram_ids,\n",
    "            \"avg_actions_per_tick\": avg_actions_per_tick,\n",
    "            \"var_actions_per_tick\": var_actions_per_tick,\n",
    "            \"mean_actions_per_tick\": mean_actions_per_tick,\n",
    "            \"std_actions_per_tick\": std_actions_per_tick,\n",
    "            \"burstiness_actions\": burstiness_actions,\n",
    "        }, index=df.index)\n",
    "\n",
    "        if self._label_map is not None and \"util\" in df.columns:\n",
    "            Y = df[\"util\"].fillna(\"\").astype(str).apply(lambda u: self._label_map.get(u, -1)).astype(int)\n",
    "            base_df.insert(0, \"Y\", Y.values)\n",
    "\n",
    "        X = pd.concat([base_df, nav_df, tfidf_df, topic_df], axis=1)\n",
    "        return X\n",
    "\n",
    "# ----------------------- Main -----------------------\n",
    "def main():\n",
    "    print(\"üöÄ D√©marrage g√©n√©ration de features (mode VS Code)\")\n",
    "\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        raise FileNotFoundError(f\"Fichier introuvable: {INPUT_FILE}\")\n",
    "\n",
    "    df = pd.read_csv(INPUT_FILE, dtype=str).fillna(\"\")\n",
    "    if \"trace_str\" not in df.columns:\n",
    "        raise ValueError(\"Colonne 'trace_str' manquante dans le CSV.\")\n",
    "\n",
    "    print(f\"üìä Dimensions avant transformation : {df.shape[0]} lignes √ó {df.shape[1]} colonnes\")\n",
    "\n",
    "    cfg = FeatureConfig()\n",
    "    assembler = FeaturesAssembler(cfg)\n",
    "\n",
    "    if TRANSFORM_ONLY:\n",
    "        print(\"‚öôÔ∏è Mode transform-only : chargement des artefacts‚Ä¶\")\n",
    "        assembler.load(ARTIFACTS_DIR)\n",
    "    else:\n",
    "        print(\"üìå Fit artefacts‚Ä¶\")\n",
    "        assembler.fit(df, ARTIFACTS_DIR)\n",
    "        print(\"üîÑ Rechargement artefacts‚Ä¶\")\n",
    "        assembler.load(ARTIFACTS_DIR)\n",
    "\n",
    "    print(\"üõ† Transformation‚Ä¶\")\n",
    "    X = assembler.transform(df)\n",
    "\n",
    "    print(f\"‚úÖ Dimensions apr√®s transformation : {X.shape[0]} lignes √ó {X.shape[1]} colonnes\")\n",
    "\n",
    "    X.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "    print(\"üíæ Fichier de sortie enregistr√© :\", OUTPUT_FILE)\n",
    "    print(\"üìÅ Artefacts :\", ARTIFACTS_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cd0fc1",
   "metadata": {},
   "source": [
    "## Entrainement du modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfa43079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Charg√©: train_features_ready_2.csv\n",
      "üîπ Dataset : 3278 lignes √ó 78 colonnes\n",
      "   ‚Ä¢ TF-IDF colonnes     : 36\n",
      "   ‚Ä¢ Non-TF-IDF colonnes : 41\n",
      "\n",
      "===== R√©sultats (RandomForest) =====\n",
      "Accuracy    : 0.8415\n",
      "F1-Macro    : 0.8195\n",
      "F1-Weighted : 0.8298\n",
      "OOB Score   : 0.8352\n",
      "\n",
      "Classification report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1      1.000     1.000     1.000         2\n",
      "           1      1.000     1.000     1.000         2\n",
      "           2      1.000     1.000     1.000         3\n",
      "           3      1.000     0.333     0.500         3\n",
      "           4      1.000     1.000     1.000         3\n",
      "           5      1.000     1.000     1.000         2\n",
      "           6      1.000     0.667     0.800         3\n",
      "           7      0.000     0.000     0.000         1\n",
      "           8      1.000     1.000     1.000         2\n",
      "           9      1.000     0.500     0.667         2\n",
      "          10      0.500     1.000     0.667         1\n",
      "          11      1.000     1.000     1.000         3\n",
      "          12      0.833     1.000     0.909         5\n",
      "          13      1.000     1.000     1.000         2\n",
      "          14      1.000     1.000     1.000         2\n",
      "          15      1.000     1.000     1.000         2\n",
      "          16      0.750     1.000     0.857         3\n",
      "          17      0.500     0.333     0.400         3\n",
      "          18      1.000     1.000     1.000         3\n",
      "          19      1.000     1.000     1.000         2\n",
      "          20      1.000     1.000     1.000         2\n",
      "          21      1.000     1.000     1.000         2\n",
      "          22      0.636     0.778     0.700         9\n",
      "          23      0.500     1.000     0.667         1\n",
      "          24      1.000     1.000     1.000         1\n",
      "          25      1.000     1.000     1.000         2\n",
      "          26      1.000     1.000     1.000         2\n",
      "          27      1.000     1.000     1.000         2\n",
      "          28      1.000     1.000     1.000         1\n",
      "          29      1.000     1.000     1.000         1\n",
      "          30      1.000     1.000     1.000         4\n",
      "          31      1.000     1.000     1.000         1\n",
      "          32      0.500     0.500     0.500         2\n",
      "          33      0.667     0.667     0.667         3\n",
      "          34      1.000     1.000     1.000         2\n",
      "          35      0.800     1.000     0.889         4\n",
      "          36      1.000     1.000     1.000         1\n",
      "          37      1.000     0.500     0.667         4\n",
      "          38      1.000     1.000     1.000         2\n",
      "          39      1.000     1.000     1.000         2\n",
      "          40      1.000     1.000     1.000         2\n",
      "          41      0.500     1.000     0.667         1\n",
      "          42      0.667     1.000     0.800         2\n",
      "          43      1.000     1.000     1.000         3\n",
      "          44      1.000     1.000     1.000         1\n",
      "          45      1.000     1.000     1.000         1\n",
      "          46      1.000     0.667     0.800         3\n",
      "          47      1.000     1.000     1.000         3\n",
      "          48      1.000     1.000     1.000         2\n",
      "          49      1.000     1.000     1.000         3\n",
      "          50      0.500     1.000     0.667         1\n",
      "          51      0.667     0.667     0.667         3\n",
      "          52      1.000     1.000     1.000         1\n",
      "          53      1.000     0.500     0.667         2\n",
      "          54      1.000     1.000     1.000         1\n",
      "          55      0.800     0.444     0.571         9\n",
      "          56      1.000     1.000     1.000         3\n",
      "          57      0.800     1.000     0.889         4\n",
      "          58      0.556     1.000     0.714         5\n",
      "          59      1.000     0.667     0.800         3\n",
      "          60      1.000     1.000     1.000         3\n",
      "          61      1.000     0.667     0.800         6\n",
      "          62      1.000     0.857     0.923         7\n",
      "          63      1.000     1.000     1.000         3\n",
      "          64      1.000     1.000     1.000         1\n",
      "          65      1.000     1.000     1.000         2\n",
      "          66      1.000     1.000     1.000         1\n",
      "          67      1.000     1.000     1.000         3\n",
      "          68      1.000     1.000     1.000         3\n",
      "          69      1.000     0.333     0.500         3\n",
      "          70      1.000     1.000     1.000         4\n",
      "          71      1.000     0.333     0.500         3\n",
      "          72      1.000     0.667     0.800         3\n",
      "          73      0.000     0.000     0.000         1\n",
      "          74      1.000     1.000     1.000         2\n",
      "          75      1.000     1.000     1.000         2\n",
      "          76      1.000     1.000     1.000         1\n",
      "          77      1.000     1.000     1.000         7\n",
      "          78      0.000     0.000     0.000         2\n",
      "          79      0.750     1.000     0.857         3\n",
      "          80      0.000     0.000     0.000         1\n",
      "          81      0.333     0.500     0.400         2\n",
      "          82      0.800     1.000     0.889         4\n",
      "          83      1.000     1.000     1.000         2\n",
      "          84      1.000     1.000     1.000         3\n",
      "          85      1.000     1.000     1.000         1\n",
      "          86      0.000     0.000     0.000         1\n",
      "          87      1.000     0.500     0.667         2\n",
      "          88      1.000     1.000     1.000         1\n",
      "          89      1.000     1.000     1.000         1\n",
      "          90      1.000     1.000     1.000         4\n",
      "          91      1.000     0.500     0.667         2\n",
      "          92      1.000     1.000     1.000         2\n",
      "          93      1.000     1.000     1.000         1\n",
      "          94      0.800     1.000     0.889         4\n",
      "          95      0.800     1.000     0.889         4\n",
      "          96      1.000     1.000     1.000         1\n",
      "          97      1.000     0.500     0.667         2\n",
      "          98      1.000     1.000     1.000         2\n",
      "          99      1.000     1.000     1.000         1\n",
      "         100      0.750     1.000     0.857         3\n",
      "         101      1.000     1.000     1.000         3\n",
      "         102      1.000     0.400     0.571         5\n",
      "         103      0.571     0.667     0.615         6\n",
      "         104      0.667     0.667     0.667         3\n",
      "         105      0.000     0.000     0.000         1\n",
      "         106      0.600     1.000     0.750         3\n",
      "         107      0.800     0.800     0.800         5\n",
      "         108      1.000     1.000     1.000         1\n",
      "         109      1.000     1.000     1.000         2\n",
      "         110      1.000     1.000     1.000         3\n",
      "         111      1.000     1.000     1.000         1\n",
      "         112      0.000     0.000     0.000         1\n",
      "         113      1.000     0.667     0.800         3\n",
      "         114      1.000     1.000     1.000         2\n",
      "         115      0.500     0.667     0.571         3\n",
      "         116      1.000     1.000     1.000         1\n",
      "         117      1.000     0.500     0.667         2\n",
      "         118      0.750     1.000     0.857         6\n",
      "         119      1.000     1.000     1.000         1\n",
      "         120      1.000     1.000     1.000         2\n",
      "         121      1.000     0.500     0.667         4\n",
      "         122      1.000     1.000     1.000         1\n",
      "         123      1.000     0.667     0.800         3\n",
      "         124      1.000     1.000     1.000         1\n",
      "         125      1.000     1.000     1.000         3\n",
      "         126      0.000     0.000     0.000         1\n",
      "         127      1.000     0.857     0.923         7\n",
      "         128      1.000     0.333     0.500         3\n",
      "         129      1.000     1.000     1.000         1\n",
      "         130      1.000     1.000     1.000         2\n",
      "         131      0.800     1.000     0.889         4\n",
      "         132      1.000     1.000     1.000         3\n",
      "         133      1.000     1.000     1.000         5\n",
      "         134      1.000     1.000     1.000         2\n",
      "         135      1.000     0.750     0.857         4\n",
      "         136      1.000     1.000     1.000         2\n",
      "         137      1.000     1.000     1.000         1\n",
      "         138      0.000     0.000     0.000         1\n",
      "         139      1.000     1.000     1.000         1\n",
      "         140      1.000     0.600     0.750         5\n",
      "         141      0.250     0.333     0.286         3\n",
      "         142      1.000     1.000     1.000         3\n",
      "         143      1.000     1.000     1.000         5\n",
      "         144      0.571     1.000     0.727         4\n",
      "         145      0.800     1.000     0.889         4\n",
      "         146      1.000     1.000     1.000         1\n",
      "         147      1.000     1.000     1.000         2\n",
      "         148      1.000     1.000     1.000         3\n",
      "         149      0.750     1.000     0.857         3\n",
      "         150      1.000     1.000     1.000         2\n",
      "         151      0.667     0.667     0.667         3\n",
      "         152      1.000     1.000     1.000         1\n",
      "         153      0.667     1.000     0.800         2\n",
      "         154      1.000     1.000     1.000         2\n",
      "         155      0.000     0.000     0.000         1\n",
      "         156      1.000     1.000     1.000         2\n",
      "         157      1.000     1.000     1.000         3\n",
      "         158      1.000     1.000     1.000         3\n",
      "         159      1.000     1.000     1.000         3\n",
      "         160      0.750     0.750     0.750         4\n",
      "         161      1.000     1.000     1.000         1\n",
      "         162      0.750     1.000     0.857         3\n",
      "         163      0.000     0.000     0.000         1\n",
      "         164      1.000     1.000     1.000         3\n",
      "         165      0.000     0.000     0.000         1\n",
      "         166      1.000     1.000     1.000         2\n",
      "         167      0.750     1.000     0.857         3\n",
      "         168      0.500     1.000     0.667         1\n",
      "         169      0.700     1.000     0.824         7\n",
      "         170      1.000     1.000     1.000         4\n",
      "         171      0.591     0.867     0.703        15\n",
      "         172      0.750     0.857     0.800        14\n",
      "         173      1.000     1.000     1.000         2\n",
      "         174      1.000     1.000     1.000         4\n",
      "         175      1.000     1.000     1.000         1\n",
      "         176      0.500     0.750     0.600         4\n",
      "         177      0.667     1.000     0.800         2\n",
      "         178      1.000     1.000     1.000         5\n",
      "         179      0.500     0.667     0.571         3\n",
      "         180      1.000     1.000     1.000         3\n",
      "         181      1.000     0.667     0.800         3\n",
      "         182      1.000     1.000     1.000         3\n",
      "         183      1.000     1.000     1.000         4\n",
      "         184      1.000     0.500     0.667         2\n",
      "         185      1.000     0.500     0.667         2\n",
      "         186      0.000     0.000     0.000         1\n",
      "         187      1.000     1.000     1.000         4\n",
      "         188      1.000     1.000     1.000         2\n",
      "         189      1.000     0.667     0.800         3\n",
      "         190      1.000     1.000     1.000         3\n",
      "         191      1.000     1.000     1.000         3\n",
      "         192      1.000     1.000     1.000         2\n",
      "         193      0.500     1.000     0.667         1\n",
      "         194      1.000     1.000     1.000         2\n",
      "         195      1.000     0.750     0.857         4\n",
      "         196      1.000     1.000     1.000         1\n",
      "         197      1.000     1.000     1.000         1\n",
      "         198      1.000     1.000     1.000         3\n",
      "         199      1.000     0.500     0.667         2\n",
      "         200      0.000     0.000     0.000         1\n",
      "         201      1.000     1.000     1.000         2\n",
      "         202      1.000     1.000     1.000         1\n",
      "         203      0.500     0.500     0.500         2\n",
      "         204      1.000     1.000     1.000         3\n",
      "         205      1.000     0.333     0.500         3\n",
      "         206      0.875     1.000     0.933         7\n",
      "         207      1.000     1.000     1.000         2\n",
      "         208      0.833     1.000     0.909         5\n",
      "         209      1.000     1.000     1.000         1\n",
      "         210      1.000     1.000     1.000         1\n",
      "         211      1.000     1.000     1.000         1\n",
      "         212      1.000     0.500     0.667         2\n",
      "         213      0.500     1.000     0.667         1\n",
      "         214      1.000     0.500     0.667         2\n",
      "         215      0.250     0.500     0.333         2\n",
      "         216      0.750     1.000     0.857         3\n",
      "         217      0.750     0.750     0.750         4\n",
      "         218      0.429     0.750     0.545         4\n",
      "         219      0.800     1.000     0.889         4\n",
      "         220      1.000     1.000     1.000         3\n",
      "         221      1.000     1.000     1.000         2\n",
      "         222      0.500     1.000     0.667         2\n",
      "         223      1.000     1.000     1.000         2\n",
      "         224      1.000     1.000     1.000         3\n",
      "         225      0.333     1.000     0.500         1\n",
      "         226      1.000     1.000     1.000         4\n",
      "         227      0.000     0.000     0.000         3\n",
      "         228      1.000     1.000     1.000         3\n",
      "         229      1.000     1.000     1.000         1\n",
      "         230      0.500     1.000     0.667         1\n",
      "         231      0.000     0.000     0.000         2\n",
      "         232      1.000     1.000     1.000         1\n",
      "         233      1.000     1.000     1.000         2\n",
      "         234      1.000     1.000     1.000         3\n",
      "         235      1.000     0.667     0.800         3\n",
      "         236      1.000     1.000     1.000         1\n",
      "         237      0.500     0.333     0.400         3\n",
      "         238      0.667     0.667     0.667         3\n",
      "         239      1.000     1.000     1.000         3\n",
      "         240      1.000     1.000     1.000         1\n",
      "         241      0.000     0.000     0.000         2\n",
      "         242      1.000     0.667     0.800         3\n",
      "         243      1.000     0.667     0.800         3\n",
      "         244      1.000     1.000     1.000         2\n",
      "         245      1.000     1.000     1.000         3\n",
      "         246      1.000     1.000     1.000         4\n",
      "\n",
      "    accuracy                          0.841       656\n",
      "   macro avg      0.842     0.832     0.819       656\n",
      "weighted avg      0.855     0.841     0.830       656\n",
      "\n",
      "\n",
      "üß© Matrice de confusion :\n",
      "[[2 0 0 ... 0 0 0]\n",
      " [0 2 0 ... 0 0 0]\n",
      " [0 0 3 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 2 0 0]\n",
      " [0 0 0 ... 0 3 0]\n",
      " [0 0 0 ... 0 0 4]]\n",
      "\n",
      "üèÖ Top 25 features (importance RF) :\n",
      "                                            feature  importance\n",
      "                                    tfidf_Raccourci    0.041236\n",
      "tfidf_Clic sur une grille d'historique de recherche    0.038623\n",
      "                                  tfidf_Double-clic    0.037974\n",
      "                         tfidf_Affichage d'un toast    0.036977\n",
      "               tfidf_Lancement d'un tableau de bord    0.033663\n",
      "                               tfidf_Filtrage / Tri    0.032696\n",
      "                          tfidf_Retour sur un √©cran    0.028238\n",
      "                                  back_action_ratio    0.028197\n",
      "                          tfidf_S√©lection d'un flag    0.026706\n",
      "                               chaine_plus_frequent    0.026318\n",
      "                         tfidf_S√©lection d'un √©cran    0.021917\n",
      "                                     tfidf_Chainage    0.021036\n",
      "             tfidf_Lancement d'une action g√©n√©rique    0.020929\n",
      "                        tfidf_S√©lection d‚Äôun onglet    0.019916\n",
      "                                   ratio_navigation    0.019660\n",
      "                        tfidf_Ex√©cution d'un bouton    0.019572\n",
      "          tfidf_Entr√©e en saisie dans un formulaire    0.018532\n",
      "                                ecran_plus_frequent    0.018285\n",
      "                         tfidf_Lancement d'une stat    0.018241\n",
      "                                       ratio_saisie    0.017978\n",
      "                                       repeat_ratio    0.017448\n",
      "                                      ratio_boutons    0.016875\n",
      "                          tfidf_Cr√©ation d'un √©cran    0.016541\n",
      "                                    nb_back_actions    0.015472\n",
      "                                    ratio_dialogues    0.015311\n",
      "\n",
      "üèÖ Top 25 TF-IDF :\n",
      "                                            feature  importance\n",
      "                                    tfidf_Raccourci    0.041236\n",
      "tfidf_Clic sur une grille d'historique de recherche    0.038623\n",
      "                                  tfidf_Double-clic    0.037974\n",
      "                         tfidf_Affichage d'un toast    0.036977\n",
      "               tfidf_Lancement d'un tableau de bord    0.033663\n",
      "                               tfidf_Filtrage / Tri    0.032696\n",
      "                          tfidf_Retour sur un √©cran    0.028238\n",
      "                          tfidf_S√©lection d'un flag    0.026706\n",
      "                         tfidf_S√©lection d'un √©cran    0.021917\n",
      "                                     tfidf_Chainage    0.021036\n",
      "             tfidf_Lancement d'une action g√©n√©rique    0.020929\n",
      "                        tfidf_S√©lection d‚Äôun onglet    0.019916\n",
      "                        tfidf_Ex√©cution d'un bouton    0.019572\n",
      "          tfidf_Entr√©e en saisie dans un formulaire    0.018532\n",
      "                         tfidf_Lancement d'une stat    0.018241\n",
      "                          tfidf_Cr√©ation d'un √©cran    0.016541\n",
      "                     tfidf_Fermeture d'une dialogue    0.015236\n",
      "                     tfidf_Affichage d'une dialogue    0.015121\n",
      "                         tfidf_Saisie dans un champ    0.014841\n",
      "                              tfidf_Action de table    0.012651\n",
      "                         tfidf_Fermeture d'un panel    0.009490\n",
      "                        tfidf_Clic sur une checkbox    0.009102\n",
      "                         tfidf_Fermeture de session    0.008825\n",
      "                       tfidf_Affichage d'une erreur    0.008435\n",
      "                        tfidf_D√©s√©lection d'un flag    0.008330\n"
     ]
    }
   ],
   "source": [
    "# rf_eval_from_ready_plus.py\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# INPUT = \"train_features_ready.csv\"\n",
    "INPUT = \"train_features_ready_2.csv\"\n",
    "\n",
    "TEST_SIZE = 0.20\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "RF_KW = dict(\n",
    "    n_estimators=800,\n",
    "    criterion=\"gini\",\n",
    "    max_depth=None,\n",
    "    max_features=\"sqrt\",\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=2,\n",
    "    bootstrap=True,\n",
    "    oob_score=True,                    # ‚¨ÖÔ∏è estimation OOB\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    class_weight=\"balanced_subsample\",\n",
    ")\n",
    "\n",
    "def ensure_numeric(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in X.columns:\n",
    "        if not np.issubdtype(X[c].dtype, np.number):\n",
    "            X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "    return X.fillna(0.0)\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(INPUT):\n",
    "        raise FileNotFoundError(f\"Fichier introuvable: {INPUT}\")\n",
    "\n",
    "    df = pd.read_csv(INPUT, low_memory=False)\n",
    "    print(f\"üì• Charg√©: {INPUT}\")\n",
    "    print(f\"üîπ Dataset : {df.shape[0]} lignes √ó {df.shape[1]} colonnes\")\n",
    "\n",
    "    if \"Y\" not in df.columns:\n",
    "        raise ValueError(\"La colonne 'Y' (labels) est absente du CSV.\")\n",
    "\n",
    "    y = df[\"Y\"].astype(int).values\n",
    "    X = df.drop(columns=[\"Y\"])\n",
    "    X = ensure_numeric(X)\n",
    "\n",
    "    n_tfidf = int(np.sum(X.columns.str.startswith(\"tfidf_\")))\n",
    "    print(f\"   ‚Ä¢ TF-IDF colonnes     : {n_tfidf}\")\n",
    "    print(f\"   ‚Ä¢ Non-TF-IDF colonnes : {X.shape[1] - n_tfidf}\")\n",
    "\n",
    "    # ===== Split simple =====\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "\n",
    "    rf = RandomForestClassifier(**RF_KW)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1_macro = f1_score(y_val, y_pred, average=\"macro\")\n",
    "    f1_weighted = f1_score(y_val, y_pred, average=\"weighted\")\n",
    "\n",
    "    print(\"\\n===== R√©sultats (RandomForest) =====\")\n",
    "    print(f\"Accuracy    : {acc:.4f}\")\n",
    "    print(f\"F1-Macro    : {f1_macro:.4f}\")\n",
    "    print(f\"F1-Weighted : {f1_weighted:.4f}\")\n",
    "    print(f\"OOB Score   : {getattr(rf, 'oob_score_', np.nan):.4f}\")\n",
    "    print(\"\\nClassification report :\")\n",
    "    print(classification_report(y_val, y_pred, digits=3, zero_division=0))\n",
    "\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    print(\"\\nüß© Matrice de confusion :\")\n",
    "    print(cm)\n",
    "\n",
    "    importances = rf.feature_importances_\n",
    "    imp_df = pd.DataFrame({\"feature\": X.columns, \"importance\": importances})\n",
    "    imp_df = imp_df.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\nüèÖ Top 25 features (importance RF) :\")\n",
    "    print(imp_df.head(25).to_string(index=False))\n",
    "\n",
    "    tfidf_imp = imp_df[imp_df[\"feature\"].str.startswith(\"tfidf_\")].head(25)\n",
    "    if not tfidf_imp.empty:\n",
    "        print(\"\\nüèÖ Top 25 TF-IDF :\")\n",
    "        print(tfidf_imp.to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\n‚ÑπÔ∏è Aucune feature TF-IDF dans le top 25 (ou pas de TF-IDF).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
